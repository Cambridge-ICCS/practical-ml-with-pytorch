{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Classifying penguin species with PyTorch\n",
    "\n",
    "<img src=\"https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png\" width=\"750\" />\n",
    "\n",
    "\n",
    "Artwork by @allison_horst\n",
    "\n",
    "In this exercise, we will use the python package [``palmerpenguins``](https://github.com/mcnakhaee/palmerpenguins) to supply a toy dataset containing various features and measurements of penguins.\n",
    "\n",
    "We have already created a PyTorch dataset which yields data for each of the penguins, but first we should examine the dataset and see what it contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab Setup\n",
    "Run the following cell to install the code and dependencies from github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/Cambridge-ICCS/ml-training-material@colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 -- Part (a): look at the data\n",
    "In the following code block, we import the ``load_penguins`` function from the ``palmerpenguins`` package.\n",
    "\n",
    "- Call this function, which returns a single object, and assign it to the variable ``data``.\n",
    "  - Print ``data`` and recognise that ``load_penguins`` has returned a ``pandas.DataFrame``.\n",
    "- Consider which features it might make sense to use in order to classify the species of the penguins.\n",
    "  - You can print the column titles using ``pd.DataFrame.keys()``\n",
    "  - You can also obtain useful information using ``pd.DataFrame.Series.describe()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/c19a904462482430170bfe2c718775ddb7dbb885/man/figures/culmen_depth.png\" width=\"500\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 -- Part (b): Use seaborn to plot the distribution of the penguin species in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.pairplot(data.drop(\"year\", axis=1), hue='species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### Task 1 -- Part (c): Apply umap to visualise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Drop rows with missing values\n",
    "data = data.dropna()  \n",
    "\n",
    "# Extract features\n",
    "penguin_data = data[\n",
    "    [\n",
    "        \"bill_length_mm\",\n",
    "        \"bill_depth_mm\",\n",
    "        \"flipper_length_mm\",\n",
    "        \"body_mass_g\",\n",
    "    ]\n",
    "].values \n",
    "scaled_penguin_data = StandardScaler().fit_transform(penguin_data)\n",
    "\n",
    "# Fit and transform\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "embedding = reducer.fit_transform(scaled_penguin_data)\n",
    "\n",
    "colors = sns.color_palette()\n",
    "\n",
    "for i, (species, group) in enumerate(data.groupby(\"species\")):\n",
    "    plt.scatter(\n",
    "        embedding[data.species == species, 0],\n",
    "        embedding[data.species == species, 1],\n",
    "        label=species,\n",
    "        color=colors[i],\n",
    "    )\n",
    "\n",
    "plt.gca().set_aspect(\"equal\", \"datalim\")\n",
    "plt.title(\"UMAP projection of the Penguin dataset\", fontsize=24)\n",
    "plt.xlabel(\"UMAP 1\", fontsize=18)\n",
    "plt.ylabel(\"UMAP 2\", fontsize=18)\n",
    "plt.legend(loc=\"upper right\", fontsize=10, title=\"Species\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now discuss the features we will use to classify the penguins' species, and populate the following list together:\n",
    "- ...\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Creating a ``torch.utils.data.Dataset``\n",
    "\n",
    "The penguin data reading and processing can be encapsulated in a PyTorch dataset class.\n",
    "- Why is a class representation helpful?\n",
    "  - ...\n",
    "\n",
    "All PyTorch dataset objects are subclasses of the ``torch.utils.data.Dataset`` class. To make a custom dataset, create a class which inherits from the ``Dataset`` class, implement some methods (the Python magic (or dunder) methods ``__len__`` and ``__getitem__``) and supply some data.\n",
    "\n",
    "Spoiler alert: we've done this for you already below (see ``src/ml_workshop/_penguins.py`` for a more sophisticated implementation)\n",
    "\n",
    "- Open the file ``src/ml_workshop/_penguins.py``.\n",
    "- Let's examine, and discuss, each of the methods together.\n",
    "  - ``__len__``\n",
    "    - What does the ``__len__`` method do?\n",
    "    - ...\n",
    "  - ``__getitem__``\n",
    "    - What does the ``__getitem__`` method do?\n",
    "    - ...\n",
    "- Review and discuss the class arguments.\n",
    "  - ``input_keys``— ...\n",
    "  - ``target_key``— ...\n",
    "  - ``train``— ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Any, Dict\n",
    "\n",
    "# import some useful functions here, see https://pytorch.org/docs/stable/torch.html\n",
    "# where `tensor` is used for constructing tensors,\n",
    "# and using a lower-precision float32 is advised for performance\n",
    "# Task 4: add imports here\n",
    "# from torch import tensor, float32\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from palmerpenguins import load_penguins\n",
    "\n",
    "\n",
    "class PenguinDataset(Dataset):\n",
    "    \"\"\"Penguin dataset class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_keys : List[str]\n",
    "        The column titles to use in the input feature vectors.\n",
    "    target_key : str\n",
    "        The column titles to use in the target feature vectors.\n",
    "    train : bool\n",
    "        If ``True``, this object will serve as the training set, and if\n",
    "        ``False``, the validation set.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The validation split contains 10 male and 10 female penguins of each\n",
    "    species.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_keys: List[str],\n",
    "        target_key: str,\n",
    "        train: bool,\n",
    "    ):\n",
    "        \"\"\"Build ``PenguinDataset``.\"\"\"\n",
    "        self.input_keys = input_keys\n",
    "        self.target_key = target_key\n",
    "\n",
    "        data = load_penguins()\n",
    "        data = (\n",
    "            data.loc[~data.isna().any(axis=1)]\n",
    "            .sort_values(by=sorted(data.keys()))\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        # Transform the sex field into a float, with male represented by 1.0, female by 0.0\n",
    "        data.sex = (data.sex == \"male\").astype(float)\n",
    "        self.full_df = data\n",
    "\n",
    "        valid_df = self.full_df.groupby(by=[\"species\", \"sex\"]).sample(\n",
    "            n=10,\n",
    "            random_state=123,\n",
    "        )\n",
    "        # The training items are simply the items *not* in the valid split\n",
    "        train_df = self.full_df.loc[~self.full_df.index.isin(valid_df.index)]\n",
    "\n",
    "        self.split = {\"train\": train_df, \"valid\": valid_df}[\n",
    "            \"train\" if train is True else \"valid\"\n",
    "        ]\n",
    "\n",
    "        # Build label map from the full dataset\n",
    "        unique_labels = sorted(self.full_df[self.target_key].unique())\n",
    "        self.label_map: Dict[str, int] = {\n",
    "            label: idx for idx, label in enumerate(unique_labels)\n",
    "        }\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the length of requested split.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The number of items in the dataset.\n",
    "\n",
    "        \"\"\"\n",
    "        return len(self.split)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"Return an input-target pair.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Index of the input-target pair to return.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        in_feats : Any\n",
    "            Inputs.\n",
    "        target : Any\n",
    "            Targets.\n",
    "\n",
    "        \"\"\"\n",
    "        # get the row index (idx) from the dataframe and\n",
    "        # select relevant column features (provided as input_keys)\n",
    "        feats = tuple(self.split.iloc[idx][self.input_keys])\n",
    "\n",
    "        # this gives a 'species' i.e. one of ('Gentoo',), ('Chinstrap',), or ('Adelie',)\n",
    "        tgt = self.split.iloc[idx][self.target_key]\n",
    "\n",
    "        # Task 4 -- Part (a): Convert the tuple features to PyTorch Tensors\n",
    "\n",
    "        # Task 4 -- Part (b): Convert the target (a Python integer) to a 0-D tensor (scalar tensor).\n",
    "\n",
    "\n",
    "        return feats, tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Obtaining training and validation datasets\n",
    "\n",
    "- Instantiate the penguin dataloader.\n",
    "  - Make sure you supply the correct column titles for the features and the targets.\n",
    "- Then iterate over the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = PenguinDataset(\n",
    "    input_keys=[\"bill_length_mm\", \"body_mass_g\"],\n",
    "    target_key=...,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "\n",
    "for features, target in data_set:\n",
    "    # print the features and targets here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Can we give these items to a neural network, or do they need to be transformed first?</summary>\n",
    "  <ul>\n",
    "    <li>Short answer: no, we can't just pass tuples of numbers or strings to a neural network.\n",
    "      <ul>\n",
    "        <li>We must represent these data as <code>torch.Tensor</code>s. This is the fundamental data abstraction used by PyTorch; they are the PyTorch equivalent to Numpy arrays, while also providing support for GPU acceleration. See <a href=\"https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html\">pytorch tensors documentation</a>.</li>\n",
    "        <li>The targets are tuples of strings i.e. ('Gentoo', )\n",
    "          <ul>\n",
    "            <li>One idea is to represent as categorical indices i.e.  [1] or [2] or [3]. Will this work? \n",
    "          </ul>\n",
    "        </li>\n",
    "      </ul>\n",
    "    </li>\n",
    "  </ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 -- Part (a) and (b): Convert Dataset outputs to PyTorch Tensors\n",
    "\n",
    "Modify the `PenguinDataset` class above so that the tuples of numbers are converted to PyTorch `torch.Tensor` s and the string targets are converted to indices.\n",
    "\n",
    "- Begin by importing relevant PyTorch functions.\n",
    "- Complete the `__getitem__()` function above.\n",
    "\n",
    "Then create a training and validation set.\n",
    "\n",
    "  - We allow the model to learn directly from the training set—i.e. we fit the function to these data.\n",
    "  - During training, we monitor the model's performance on the validation set in order to check how it's doing on unseen data. Normally, people use the validation performance to determine when to stop the training process.\n",
    "  \n",
    "For the validation set, we choose ten males and ten females of each species. This means the validation set is less likely to be biased by sex and species, and is potentially a more reliable measure of performance. You should always be _very_ careful when choosing metrics and splitting data.\n",
    "\n",
    "- Is this transformation approach general? No, but it's a good start. \n",
    "  - Switch between validation/train time transformations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete __getitem__() function\n",
    "# See Task 4 ``TODOs`` in PenguinDataset class.\n",
    "\n",
    "# Create train_set\n",
    "\n",
    "# Create valid_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Task 4 -- Part (c): `torchvision.transforms.Compose`\n",
    "\n",
    "<details>\n",
    "<summary>Transforming inputs: An idiomatic approach </summary>\n",
    "\n",
    "A common way of transforming inputs to neural networks is to apply a series of transforms using `torchvision.transforms.Compose`. The [ `Compose` ](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html) object takes a list of callable objects and applies them to the incoming data. \n",
    "\n",
    "One flaw of the current implementation is that the transformations are hardcoded inside the `PenguinDataset` class, specifically in the `__getitem__` method, rather than being passed as arguments. This makes the class less general and less reusable.\n",
    "\n",
    "A more idiomatic approach in PyTorch would allow the `transform` (and optionally `target_transform`) to be passed in during initialization. This gives users more flexibility to adjust preprocessing pipelines for different use cases like training vs. validation or different model types.\n",
    "\n",
    "See how this is done more generally in the [src/ml_workshop/_penguins.py](../src/ml_workshop/_penguins.py) file. \n",
    "\n",
    "These transformations are especially useful for tasks like converting file paths to image tensors, normalization, data augmentation, and more.\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "Instantiate the `torchvision.transforms.Compose` transformations and pass to the `PenguinsDataset` in [src/ml_workshop/_penguins.py](../src/ml_workshop/_penguins.py), instead of hardcoding as above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose\n",
    "\n",
    "# from ml_workshop import PenguinDataset\n",
    "\n",
    "# import some useful functions here, see https://pytorch.org/docs/stable/torch.html\n",
    "# where `tensor` is used for constructing tensors,\n",
    "# and using a lower-precision float32 is advised for performance\n",
    "from torch import tensor, float32\n",
    "\n",
    "# Apply the transforms we need to the PenguinDataset to get out input\n",
    "# targets as Tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Creating ``DataLoaders``—and why\n",
    "\n",
    "Once we have created a ``Dataset`` object, we wrap it in a ``DataLoader``. This comes with a number of useful features:\n",
    "#### Mini-batches\n",
    "The ``DataLoader`` object allows us to put our inputs and targets in **mini-batches**, which makes for more efficient training.\n",
    "- Note: rather than supplying one input-target pair to the model at a time, we supply \"mini-batches\" of these data at once (typically a small power of 2, like 16 or 32).\n",
    "- The number of items we supply at once is called the `batch size`.\n",
    "> **Q:** What number should we choose for the batch size?\n",
    " \n",
    "#### Shuffling\n",
    "The ``DataLoader`` can also randomly **shuffle** the data each epoch (when training). \n",
    "- This avoids unwanted patterns in the data harming the fitting process. Consider providing lots of the positive class followed by the negative class, the network will only learn by saying yes all the time. Therefore need to intersperse positives and negatives.\n",
    "\n",
    "#### Parallel loading\n",
    "The ``DataLoader`` also allows **parallel loading** of mini-batches.\n",
    "- This setup is especially useful for large datasets or image files that can't all be loaded into memory at once. The DataLoader loads data in small batches as needed, and using multiple workers allows this to happen in parallel with model training—improving overall speed.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Why do we need a DataLoader?</strong></summary>\n",
    "<p>\n",
    "The <code>DataLoader</code> is a PyTorch utility that provides an iterable over the dataset, allowing us to easily access mini-batches of data. It handles batching, shuffling, and parallel loading of the data, which is essential for efficient training of neural networks.\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><strong>Note on batch normalisation</strong></summary>\n",
    "<p>\n",
    "We are going to use batch normalisation layers in our network, which won't work if the batch size is one. This can happen on the last batch, if we don't choose a batch size that evenly divides the number of items in the dataset. To avoid this, we can set the <code>drop_last</code> argument to <code>True</code>. The last batch, which will be of size <code>len(data_set) % batch_size</code>, gets dropped, and the data are reshuffled. This is only relevant during the training process — validation will use population statistics.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create training and validation DataLoaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 -- Part (a): Creating a neural network in PyTorch\n",
    "Here we will create our neural network in PyTorch, and have a general discussion on clean and messy ways of going about it.\n",
    "\n",
    "The module `torch.nn` contains different classes that help you build neural network models. All models in PyTorch inherit from the subclass `nn.Module`, which has useful methods like `parameters()`, `__call__()`, and others.\n",
    "\n",
    "`torch.nn` also has various layers that you can use to build your neural network. For example, we will use `nn.Linear` in our code below, which constructs a fully connected layer. `torch.nn.Linear` is a subclass of `torch.nn.Module`.\n",
    "\n",
    "What exactly is a \"layer\"? It is essentially a step in the neural network computation. For example, the `nn.Linear` layer computes the linear transformation of the input vector $x$:\n",
    "\n",
    "$$\n",
    "y = W^T x + b\n",
    "$$\n",
    "\n",
    "where $W$ is the matrix of tunable parameters and $b$ is a bias vector.\n",
    "\n",
    "We can also think of the ReLU activation as a \"layer\". However, there are no tunable parameters associated with the ReLU activation function.\n",
    "\n",
    "The `__init__()` method is where we typically define the attributes of a class. In our case, all the \"sub-components\" of our model should be defined here.\n",
    "\n",
    "The `forward` method is called when we use the neural network to make a prediction. Another term for \"making a prediction\" is **running the forward pass**, because information flows forward from the input through the hidden layers to the output. This builds a computational graph.\n",
    "\n",
    "The `forward` method is called from the `__call__()` function of `nn.Module`, so that when we run `model(batch)`, the `forward` method is called.\n",
    "\n",
    "### Summary of what we will do:\n",
    "- First, we will create quite an **ugly network** to highlight how to make a neural network in PyTorch at a very basic level.\n",
    "- We will then utilise `torch.nn.Sequential` as a **neater approach**.\n",
    "- Finally, we will discuss how the **best approach** would be to write a class where various parameters (e.g. number of layers, dropout probabilities, etc.) are passed as arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch.nn import BatchNorm1d, Linear, ReLU, Dropout\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class FCNet(Module):\n",
    "    \"\"\"Fully-connected neural network.\"\"\"\n",
    "\n",
    "    # define __init__ function - model defined here.\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # define forward function which calls network\n",
    "    def forward(self, batch: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "\n",
    "# define a model and print and test (try with torch.rand() function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Task 6 -- Part (b): Import ResNet50 weights\n",
    "\n",
    "Have a go at importing the model weights for a large model like ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ResNet50 here\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "# Create a ResNet50 model and print it.\n",
    "res_model = resnet50(pretrained=True)\n",
    "# Set the model to evaluation mode.\n",
    "res_model.eval()\n",
    "# Print the model architecture.\n",
    "# print(res_model)\n",
    "\n",
    "list(res_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Selecting a loss function\n",
    "\n",
    "- Cross-entropy is about the most common loss function for classification.\n",
    "  - Details on this loss function are available in the [PyTorch docs](https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n",
    "- Let's instantiate it together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Selecting an optimiser\n",
    "\n",
    "While we talked about stochastic gradient descent in the slides, most people use the so-called [Adam optimiser](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html).\n",
    "\n",
    "You can think of it as a more complex and improved implementation of SGD.\n",
    "\n",
    "Here we will tell the optimiser what parameters to fit in order to minimise the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimiser and give it the model's parameters.\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Writing basic training and validation loops\n",
    "\n",
    "####  Applying an activation function to model outputs\n",
    "\n",
    "Our model produces raw scores, called **logits**. These are unnormalized and can be positive or negative. On their own, logits are not directly interpretable.\n",
    "\n",
    "To make sense of these outputs (e.g., “how confident is the model?”), we apply an **activation function** like `softmax`, which converts logits into **pseudo-probabilities**.\n",
    "\n",
    "> **Important:** We do **not** apply this activation inside the model or during\n",
    "> training. The loss function we’re using (`CrossEntropyLoss`) expects **raw logits** and handles this internally.  \n",
    "> We only apply `softmax` **after training**, for tasks like evaluation or\n",
    "> visualization.\n",
    "\n",
    "In general, we don’t usually include the output activation function in the network itself because it’s often handled more efficiently and flexibly by the loss function. Keeping it separate makes it easier to swap in different losses or interpret raw logits directly during training and debugging.\n",
    "\n",
    "<details>\n",
    "<summary>Why softmax? And what’s this about thermodynamics?</summary>\n",
    "\n",
    "- According to the\n",
    "  [PyTorch documentation on `CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html),\n",
    "  this loss combines `LogSoftmax` and `NLLLoss` into a single function — so we\n",
    "  don’t need to apply softmax ourselves during training.\n",
    "- [Documentation on `nn.Softmax`](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)\n",
    "  shows that it's appropriate when the prediction should be **exactly one of**\n",
    "  the possible classes — like species `Adelie`, `Chinstrap`, or `Gentoo`.\n",
    "\n",
    "### What does softmax do?\n",
    "\n",
    "Softmax turns logits into probabilities — it's what lets us say \"the model is 90% confident it's a penguin of species `Adelie`.\"\n",
    "\n",
    "### Physics connection\n",
    "\n",
    "If you’ve studied physics, it might remind you of the **partition function** in thermodynamics — softmax turns logits into a normalised probability distribution, just as a partition function normalises energy states.  This analogy appears in several places in machine learning under names like \"temperature scaling\" and \"energy-based models\".\n",
    "\n",
    "### When not to use softmax?\n",
    "This activation is **not appropriate** if you want to assign **multiple labels** to the same input (e.g. both \"dog\" _and_ \"cat\").\n",
    "\n",
    "### Final note on softmax and argmax\n",
    "Since softmax is **monotonic** (it preserves order), using `argmax` on logits gives the same result as `argmax` on softmax probabilities.  \n",
    "That’s why we don’t need to apply softmax just to get the predicted class.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Part (a) -- training and validation loops\n",
    "\n",
    "Have a go at writing the training and evaluation loops. Use the tips below to guide you.\n",
    "\n",
    "#### Tips\n",
    "\n",
    "- Configure the model separately for **training** and **validation**.\n",
    "- Don't worry about storing metrics just yet, focus on the training and validation loops.\n",
    "- Try to get it training first before implementing the validation loop. We will guide you through this process. \n",
    "\n",
    "<details>\n",
    "<summary>How does the loss **know** about parameters in the model? </summary>\n",
    "\n",
    "- PyTorch tensors can track operations when `requires_grad=True`.\n",
    "- When we run the **forward pass** (`preds = model(batch)`), PyTorch builds a **dynamic computation graph** that records all operations, including how the model's parameters were used to compute the predictions.\n",
    "- The **loss** (`loss = criterion(preds, targets)`) depends on the predictions, so it's also connected to the model parameters via this graph.\n",
    "- Calling `loss.backward()` performs the **backward pass**, where PyTorch:\n",
    "  - Traverses the computation graph,\n",
    "  - Computes gradients of the loss w.r.t. each parameter that requires gradients.\n",
    "\n",
    "> `loss.backward()` **computes gradients but does not update** parameters. You need to call `optimizer.step()` to apply the updates.\n",
    "\n",
    "Because PyTorch uses a **dynamic graph**, the graph is recreated on every forward pass, making it flexible and easy to debug.\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>PyTorch Autograd and <code>requires_grad</code></summary>\n",
    "\n",
    "**NOTE:** In PyTorch, `requires_grad=True` is set automatically for the parameters of layers defined using `torch.nn.Module` subclasses.\n",
    "\n",
    "Here’s a simple example:\n",
    "\n",
    "```python\n",
    "x = ones(10, requires_grad=True)\n",
    "y = 2 * x.exp()\n",
    "print(y)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Don't forget: Optimizer accumulates gradients</summary>\n",
    "\n",
    "By default, PyTorch accumulates gradients each time you call `loss.backward()`.  \n",
    "You must reset them manually with `optimizer.zero_grad()` before each training step.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Evaluation tip: Disable gradients</summary>\n",
    "\n",
    "Use the decorator  `@no_grad()` or context `with no_grad():` during evaluation or inference.  \n",
    "This saves memory and speeds things up by skipping gradient tracking.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Part (b) -- tracking metrics\n",
    "\n",
    "To track training and validation progress, follow these steps:\n",
    "\n",
    "- Define a dictionary:\n",
    "  ```python\n",
    "  metrics = {\"loss\": [], \"accuracy\": []}\n",
    "  ```\n",
    "\n",
    "- Append the loss using `loss.item()` (it's a scalar; no gradients needed).\n",
    "\n",
    "- Write a function `get_batch_accuracy(preds: Tensor, targets: Tensor)`.\n",
    "\n",
    "- We need to supply the metrics as `means` over each epoch.\n",
    " <details>\n",
    " <summary> More on metrics structure</summary>\n",
    "\n",
    " The metrics should be a dictionary containing `\"loss\"` and `\"accuracy\"` as keys and lists as values which we append to during each iteration.  \n",
    "\n",
    " We can then use dictionary comprehension to get epoch-level statistics:\n",
    " ```python\n",
    " epoch_stats = {k: mean(v) for k, v in metrics.items()}\n",
    " ```\n",
    "</details> \n",
    "\n",
    "<details>\n",
    "<summary>Example: Summarising metrics over an epoch</summary>\n",
    "\n",
    "```python\n",
    "# i.e. for a batch_size of 3\n",
    "metrics = {\"loss\": [1.0, 2.0, 3.0], \"accuracy\": [0.7, 0.8, 0.9]}\n",
    "epoch_stats = {k: mean(v) for k, v in metrics.items()}\n",
    "```\n",
    "</details> <details> <summary>Over-fitting warning</summary>\n",
    "\n",
    "If the validation performance gets significantly worse while training performance improves, it’s a sign of over-fitting.\n",
    "\n",
    "</details> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: Module,\n",
    "    train_loader: DataLoader,\n",
    "    optimiser: Adam,\n",
    "    loss_func: CrossEntropyLoss,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Train ``model`` for once epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Module\n",
    "        The neural network.\n",
    "    train_loader : DataLoader\n",
    "        Training dataloader.\n",
    "    optimiser : Adam\n",
    "        The optimiser.\n",
    "    loss_func : CrossEntropyLoss\n",
    "        Cross-entropy loss function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        A dictionary of metrics.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # setup the model for training. IMPORTANT!\n",
    "\n",
    "    # setup loss and accuracy metrics dictionary\n",
    "\n",
    "    # iterate over the batch, targets in the train_loader\n",
    "    for batch, targets in train_loader:\n",
    "        pass\n",
    "\n",
    "        # zero the gradients (otherwise gradients accumulate)\n",
    "\n",
    "        # run forward model and compute proxy probabilities over dimension 1 (columns of tensor).\n",
    "\n",
    "        # compute loss\n",
    "        # e.g. pred : Tensor([3]) and target : int\n",
    "\n",
    "        # compute gradients\n",
    "\n",
    "        # nudge parameters in direction of steepest descent c\n",
    "\n",
    "        # append metrics\n",
    "\n",
    "\n",
    "def validate_one_epoch(\n",
    "    model: Module,\n",
    "    valid_loader: DataLoader,\n",
    "    loss_func: CrossEntropyLoss,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Validate ``model`` for a single epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Module\n",
    "        The neural network.\n",
    "    valid_loader : DataLoader\n",
    "        Validation dataloader.\n",
    "    loss_func : CrossEntropyLoss\n",
    "        Cross-entropy loss function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        Metrics of interest.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for batch, targets in valid_loader:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Training, extracting and plotting metrics\n",
    "\n",
    "- Now we can train our model for a specified number of epochs.\n",
    "  - During each epoch the model \"sees\" each training item once.\n",
    "- Append the training and validation metrics to a list.\n",
    "- Turn them into a ``pandas.DataFrame``\n",
    "  - Note: You can turn a ``List[Dict[str, float]]``, say ``my_list`` into a ``DataFrame`` with ``DataFrame(my_list)``.\n",
    "- Use Matplotlib to plot the training and validation metrics as a function of the number of epochs.\n",
    "\n",
    "We will begin the code block together before you complete it independently.  \n",
    "After some time we will go through the solution together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "# define train_metrics and valid_metrics lists.\n",
    "\n",
    "for _ in range(epochs):\n",
    "\n",
    "    # append output of train_one_epoch() to train_metrics\n",
    "\n",
    "    # append output of valid_one_epoch() to valid_metrics\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11: Visualise some results\n",
    "\n",
    "Let's do this part together—though feel free to make a start on your own if you have completed the previous exercises.\n",
    "\n",
    "<details>\n",
    "<summary>Visualising results</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "quantities = [\"loss\", \"accuracy\"]\n",
    "splits = [\"train\", \"valid\"]\n",
    "\n",
    "epochs_range = np.arange(1, epochs + 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "for i, quant in enumerate(quantities):\n",
    "    ax = axes[i]\n",
    "    for split in splits:\n",
    "        values = metrics[f\"{quant}_{split}\"]\n",
    "        ax.plot(epochs_range, values, marker='o', markersize=2, label=split.capitalize())\n",
    "    ax.set_title(quant.capitalize())\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(quant.capitalize())\n",
    "    ax.set_xlim(1, epochs)\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "quantities = [\"loss\", \"accuracy\"]\n",
    "splits = [\"train\", \"valid\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 12 -- Part (a): Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class_names = sorted(data.species.unique())\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with no_grad():\n",
    "    for batch, label in valid_loader:\n",
    "        preds = model(batch).softmax(dim=1)\n",
    "        all_preds.append(preds.argmax(dim=1).numpy())\n",
    "        all_labels.append(label.numpy())\n",
    "\n",
    "# concatenate all predictions and labels\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=[0, 1, 2])\n",
    "cm_normalized = cm.astype(\"float\") / (cm.sum(axis=1)[:, np.newaxis] + 1e-8)\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm_normalized, display_labels=class_names\n",
    ")\n",
    "\n",
    "# plotting\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "disp.plot(ax=ax, cmap=\"Blues\", colorbar=True, values_format=\".2f\")\n",
    "disp.ax_.set_xlabel(\"Predicted Label\")\n",
    "disp.ax_.set_ylabel(\"True Label\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(False)  # cleaner plot\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 12 -- Part (b): Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# class_names = ['Adelie', 'Chinstrap', 'Gentoo']\n",
    "report = classification_report(\n",
    "    y_true=all_labels,\n",
    "    y_pred=all_preds,\n",
    "    target_names=class_names,\n",
    "    output_dict=True  # <- so we can plot it\n",
    ")\n",
    "\n",
    "\n",
    "# Convert the report dict to DataFrame for plotting\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df = report_df.loc[class_names, ['precision', 'recall', 'f1-score']]\n",
    "\n",
    "# Plot\n",
    "report_df.plot(kind='bar', figsize=(8, 5))\n",
    "plt.title(\"Per-Class Precision, Recall, and F1 Score\")\n",
    "plt.ylim(0.0, 1.05)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Run the net on 'new' inputs\n",
    "\n",
    "We have built and trained a net, and evaluated and visualised its performance. However, how do we now utilise it going forward?\n",
    "\n",
    "Here we construct some 'new' input data and use our trained net to infer the species. Whilst this is relatively straightforward there is still some work required to transform the outputs from the net to a meaningful result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import no_grad\n",
    "\n",
    "# Construct a tensor of inputs to run the model over\n",
    "\n",
    "# Place model in eval mode and run over inputs with no_grad\n",
    "\n",
    "# Print the raw output from the net\n",
    "\n",
    "# Transform the raw output back to human-readable format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
