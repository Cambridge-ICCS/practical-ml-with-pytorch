{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Classifying penguin species with PyTorch\n",
    "\n",
    "<img src=\"https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png\" width=\"750\" />\n",
    "\n",
    "\n",
    "Artwork by @allison_horst\n",
    "\n",
    "In this exercise, we will use the python package [``palmerpenguins``](https://github.com/mcnakhaee/palmerpenguins) to supply a toy dataset containing various features and measurements of penguins.\n",
    "\n",
    "We have already created a PyTorch dataset which yields data for each of the penguins, but first we should examine the dataset and see what it contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: look at the data\n",
    "In the following code block, we import the ``load_penguins`` function from the ``palmerpenguins`` package.\n",
    "\n",
    "- Call this function, which returns a single object, and assign it to the variable ``data``.\n",
    "  - Print ``data`` and recognise that ``load_penguins`` has returned a ``pandas.DataFrame``.\n",
    "- Consider which features it might make sense to use in order to classify the species of the penguins.\n",
    "  - You can print the column titles using ``pd.DataFrame.keys()``\n",
    "  - You can also obtain useful information using ``pd.DataFrame.Series.describe()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "\n",
    "data = load_penguins()\n",
    "\n",
    "# Note: ``pd.DataFrame.describe`` is a useful function for giving an overview\n",
    "# of what a ``pd.DataFrame`` contains.\n",
    "print(data.describe())\n",
    "\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now discuss the features we will use to classify the penguins' species, and populate the following list together:\n",
    "\n",
    "#### Let's use:\n",
    "\n",
    "- ``\"bill_length_mm\"``\n",
    "    - Biologically relevant and unambiguous.\n",
    "- ``\"bill_depth_mm\"``\n",
    "    - Biologically relevant.\n",
    "- ``flipper_length_mm``\n",
    "    - Biologically relevant.\n",
    "- ``\"body_mass_g\"``\n",
    "    - Biologically relevant.\n",
    "- ``\"sex\"``\n",
    "    - While a potential source of bias, it is likely informative and biologically relevant.\n",
    "\n",
    "#### Let's reject\n",
    "- ``\"island\"``\n",
    "    - While island is likely to be predictive, it seems potentially misleading to use this feature. One island could be heavily dominated by one species of penguin, while other species abide there in much smaller numbers. Such a situation could result in a model giving too much weight to this feature, and confounding the results.\n",
    "- ``\"year\"``\n",
    "    - This feature could also be important: then behaviour of certain species may be changing in response to time-dependent environmental factors such as melting ice. It does however seem like the least biologically-relevant feature, and the most likely source of bias, so we reject it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: creating a ``torch.utils.data.Dataset``\n",
    "\n",
    "The penguin data reading and processing can be encapsulated in a PyTorch dataset class.\n",
    "\n",
    "- Why is this class representation helpful?\n",
    "  - Modularity - Separation of concerns makes the cde easier to understand, maintain and test.\n",
    "  - Maintainability - Changes are localised, therefore we only need to change a single file to update. \n",
    "  - Abstraction - Users do not need to know how the data is read or processed, they only need to know how to interact with the class. \n",
    "\n",
    "\n",
    "All PyTorch dataset objects are subclasses of the ``torch.utils.data.Dataset`` class. To make a custom dataset, create a class which inherits from the ``Dataset`` class, implement some methods (the Python magic (or dunder) methods ``__len__`` and ``__getitem__``) and supply some data.\n",
    "\n",
    "Spoiler alert: we've done this for you already in ``src/ml_workshop/_penguins.py``.\n",
    "\n",
    "- Open the file ``src/ml_workshop/_penguins.py``.\n",
    "- Let's examine, and discuss, each of the methods together.\n",
    "  - ``__len__``\n",
    "    - What does the ``__len__`` method do?\n",
    "      - The ``__len__`` method is a so-called \"magic method\", which tells python to do if the ``len`` function is called on the object containing it.\n",
    "  - ``__getitem__``\n",
    "    - What does the ``__getitem__`` method do?\n",
    "      - The ``__getitem__`` method is another magic method which tells python what to do if we try and index the object containing it (i.e. ``my_object[idx]``).\n",
    "- Review and discuss the class arguments.\n",
    "  - ``input_keys``— A sequence of strings telling the data set which objects to return as inputs to the model.\n",
    "  - ``target_keys``— Same as ``input_keys`` but specifying the targets.\n",
    "  - ``train``— A boolean variable determining if the model returns the training or validation split (``True`` for training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict, Tuple, Any\n",
    "\n",
    "# import pytorch functions necessary for transformations:\n",
    "from torch import tensor, float32, eye\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "from palmerpenguins import load_penguins\n",
    "\n",
    "\n",
    "class PenguinDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_keys: List[str],\n",
    "        target_keys: List[str],\n",
    "        train: bool,\n",
    "    ):\n",
    "        \"\"\"Build ``PenguinDataset``.\"\"\"\n",
    "        self.input_keys = input_keys\n",
    "        self.target_keys = target_keys\n",
    "\n",
    "        data = load_penguins()\n",
    "        data = (\n",
    "            data.loc[~data.isna().any(axis=1)]\n",
    "            .sort_values(by=sorted(data.keys()))\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        # Transform the sex field into a float, with male represented by 1.0, female by 0.0\n",
    "        data.sex = (data.sex == \"male\").astype(float)\n",
    "        self.full_df = data\n",
    "\n",
    "        valid_df = self.full_df.groupby(by=[\"species\", \"sex\"]).sample(\n",
    "            n=10,\n",
    "            random_state=123,\n",
    "        )\n",
    "        # The training items are simply the items *not* in the valid split\n",
    "        train_df = self.full_df.loc[~self.full_df.index.isin(valid_df.index)]\n",
    "\n",
    "        self.split = {\"train\": train_df, \"valid\": valid_df}[\n",
    "            \"train\" if train is True else \"valid\"\n",
    "        ]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.split)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Any, Any]:\n",
    "        # get the row index (idx) from the dataframe and\n",
    "        # select relevant column features (provided as input_keys)\n",
    "        feats = tuple(self.split.iloc[idx][self.input_keys])\n",
    "\n",
    "        # this gives a 'species' i.e. one of ('Gentoo',), ('Chinstrap',), or ('Adelie',)\n",
    "        tgts = tuple(self.split.iloc[idx][self.target_keys])\n",
    "\n",
    "        # Exercise #1: convert the feats (Series) to PyTorch Tensors\n",
    "        feats = tensor(feats, dtype=float32)\n",
    "\n",
    "        # Exercise #2: convert target to a 'one-hot' vector.\n",
    "        target_names = sorted(self.full_df.species.unique())\n",
    "        tgts = eye(len(target_names))[target_names.index(tgts[0])]\n",
    "\n",
    "        return feats, tgts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: obtaining training and validation datasets\n",
    "\n",
    "- Instantiate the penguin dataloader.\n",
    "  - Make sure you supply the correct column titles for the features and the targets.\n",
    "- Iterate over the dataset\n",
    "    - Hint:\n",
    "        ```python\n",
    "        for features, targets in dataset:\n",
    "            # print the features and targets here\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"bill_length_mm\",\n",
    "    \"bill_depth_mm\",\n",
    "    \"body_mass_g\",\n",
    "    \"flipper_length_mm\",\n",
    "    \"sex\",\n",
    "]\n",
    "\n",
    "target_names = sorted(data.species.unique())\n",
    "\n",
    "data_set = PenguinDataset(\n",
    "    input_keys=features,\n",
    "    target_keys=[\"species\"],\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "for _, (input_feats, target) in zip(range(20), data_set):\n",
    "    print(input_feats, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can we give these items to a neural network, or do they need to be transformed first?\n",
    "  + Short answer: no, we can't just pass tuples of numbers or strings to a neural network.\n",
    "    - We must represent these data as ``torch.Tensor``s. This is the fundamental data abstraction used by PyTorch. See [pytorch tensors documentation](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html)\n",
    "  + The targets are tuples of strings i.e. ('Gentoo', )\n",
    "    - One idea is to represent as ordinal values i.e.  [1] or [2] or [3]. But this implies that the class encoded by value 1 is closer to 2 than 1 is to 3. This is not desirable for categorical data. One-hot encoding avoids this by representing each species independently.\\\n",
    "    \"A\" — [1, 0, 0]\\\n",
    "    \"B\" — [0, 1, 0]\\\n",
    "    \"C\" — [0, 0, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Applying transforms to the data\n",
    "\n",
    "- Here we create a training and validation set.\n",
    "    - We allow the model to learn directly from the training set—i.e. we fit the function to these data.\n",
    "    - During training, we monitor the model's performance on the validation set in order to check how it's doing on unseen data. Normally, people use the validation performance to determine when to stop the training process.\n",
    "- For the validation set, we choose ten males and ten females of each species. This means the validation set is less likely to be biased by sex and species, and is potentially a more reliable measure of performance. You should always be _very_ careful when choosing metrics and splitting data.\n",
    "\n",
    "\n",
    "Note: A common way of transforming inputs to neural networks is to apply a series of transforms using ``torchvision.transforms.Compose``. The [``Compose``](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html) object takes a list of callable objects and applies them to the incoming data. See how this is done more generally in the `src/ml_workshop/_penguins.py` file. \n",
    "\n",
    "These transforms can be very useful for mapping between file paths and tensors of images, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose\n",
    "\n",
    "# import some useful functions here, see https://pytorch.org/docs/stable/torch.html\n",
    "# where `tensor` and `eye` are used for constructing tensors,\n",
    "# and using a lower-precision float32 is advised for performance\n",
    "from torch import tensor, float32, eye\n",
    "\n",
    "train_set = PenguinDataset(\n",
    "    input_keys=features,\n",
    "    target_keys=[\"species\"],\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "\n",
    "valid_set = PenguinDataset(\n",
    "    input_keys=features,\n",
    "    target_keys=[\"species\"],\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "\n",
    "for _, (input_feats, target) in zip(range(5), train_set):\n",
    "    print(input_feats, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Creating ``DataLoaders``—and why\n",
    "\n",
    "- Once we have created a ``Dataset`` object, we wrap it in a ``DataLoader``.\n",
    "  - The ``DataLoader`` object allows us to put our inputs and targets in mini-batches, which makes for more efficient training.\n",
    "    - Note: rather than supplying one input-target pair to the model at a time, we supply \"mini-batches\" of these data at once (typically a small power of 2, like 16 or 32).\n",
    "    - The number of items we supply at once is called the batch size.\n",
    "  - The ``DataLoader`` can also randomly shuffle the data each epoch (when training).\n",
    "  - It allows us to load different mini-batches in parallel, which can be very useful for larger datasets and images that can't all fit in memory at once.\n",
    "\n",
    "\n",
    "Note: we are going to use batch normalisation layers in our network, which don't work if the batch size is one. This can happen on the last batch, if we don't choose a batch size that evenly divides the number of items in the data set. To avoid this, we can set the ``drop_last`` argument to ``True``. The last batch, which will be of size ``len(data_set) % batch_size`` gets dropped, and the data are reshuffled. This is only relevant during the training process - validation will use population statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "for batch, targets in valid_loader:\n",
    "    print(batch.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Creating a neural network in PyTorch\n",
    "\n",
    "Here we will create our neural network in PyTorch, and have a general discussion on clean and messy ways of going about it.\n",
    "\n",
    "- First, we will create quite an ugly network to highlight how to make a neural network in PyTorch on a very basic level.\n",
    "- We will then discuss a trick for making the print-out nicer.\n",
    "- Finally, we will discuss how the best approach would be to write a class where various parameters (e.g. number of layers, dropout probabilities, etc.) are passed as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch.nn import BatchNorm1d, Linear, LeakyReLU, Dropout, Sequential\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class FCNet(Module):\n",
    "    \"\"\"Fully-connected neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_feats : int\n",
    "        The number of input features to the model.\n",
    "    out_feats : int\n",
    "        The number of output features (or classes) the model should produce.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The ``torch.nn.Sequential`` class allows us to \"chain\" multiple layers, rather\n",
    "    than manually passing the output of one to the next in the forward\n",
    "    function.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_feats: int, out_feats: int):\n",
    "        \"\"\"Build the neural network.\"\"\"\n",
    "        super().__init__()\n",
    "        self._fwd_seq = Sequential(\n",
    "            BatchNorm1d(in_feats),\n",
    "            Linear(in_feats, 16),\n",
    "            BatchNorm1d(16),\n",
    "            Dropout(0.1),\n",
    "            LeakyReLU(0.1),\n",
    "            Linear(16, 16),\n",
    "            BatchNorm1d(16),\n",
    "            Dropout(0.1),\n",
    "            LeakyReLU(0.1),\n",
    "            Linear(16, out_feats),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch: Tensor) -> Tensor:\n",
    "        \"\"\"Pass ``batch`` through the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : Tensor\n",
    "            A mini-batch of inputs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            The result of passing ``batch`` through the model.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._fwd_seq(batch)\n",
    "\n",
    "\n",
    "model = FCNet(len(features), len(target_names))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Selecting a loss function\n",
    "\n",
    "- Binary cross-entropy is about the most common loss function for classification.\n",
    "  - Details on this loss function are available in the [PyTorch docs](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html).\n",
    "- Let's instantiate it together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCELoss\n",
    "\n",
    "loss_func = BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Selecting an optimiser\n",
    "\n",
    "While we talked about stochastic gradient descent in the slides, most people use the so-called [Adam optimiser](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html).\n",
    "\n",
    "You can think of it as a more complex and improved implementation of SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimiser and give it the model's parameters.\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimiser = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Writing basic training and validation loops\n",
    "\n",
    "- Before we jump in and write these loops, we must first choose an activation function to apply to the model's outputs.\n",
    "  - Here we are going to use the softmax activation function: see [the PyTorch docs](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html).\n",
    "  - For those of you who've studied physics, you may be remininded of the partition function in thermodynamics.\n",
    "  - This activation function is good for classifcation when the result is one of ``A or B or C``.\n",
    "    - It's bad if you even want to assign two classification to one images—say a photo of a dog _and_ a cat.\n",
    "  - It turns the raw outputs, or logits, into \"psuedo probabilities\", and we take our prediction to be the most probable class.\n",
    "\n",
    "- We will write the training loop together, then you can go ahead and write the (simpler) validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from numpy import mean\n",
    "\n",
    "from torch import no_grad\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: Module,\n",
    "    train_loader: DataLoader,\n",
    "    optimiser: Adam,\n",
    "    loss_func: BCELoss,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Train ``model`` for once epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Module\n",
    "        The neural network.\n",
    "    train_loader : DataLoader\n",
    "        Training dataloader.\n",
    "    optimiser : Adam\n",
    "        The optimiser.\n",
    "    loss_func : BCELoss\n",
    "        Binary cross-entropy loss function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        A dictionary of metrics.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The ``model.train()`` is very important:\n",
    "        - it turns on the dropout layers.\n",
    "        - it tells the batch norm layers to use the incoming\n",
    "          statistics, and let them contribute to their \"memory\".\n",
    "\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    metrics: Dict[str, float] = {\"loss\": [], \"accuracy\": []}\n",
    "\n",
    "    for batch, targets in train_loader:\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        preds = model(batch).softmax(dim=1)\n",
    "\n",
    "        loss = loss_func(preds, targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimiser.step()\n",
    "\n",
    "        metrics[\"loss\"].append(loss.item())\n",
    "        metrics[\"accuracy\"].append(batch_level_accuracy(preds, targets))\n",
    "\n",
    "    return {key: mean(val) for key, val in metrics.items()}\n",
    "\n",
    "\n",
    "@no_grad()\n",
    "def validate_one_epoch(\n",
    "    model: Module,\n",
    "    valid_loader: DataLoader,\n",
    "    loss_func: BCELoss,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Validate ``model`` for a single epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Module\n",
    "        The neural network.\n",
    "    valid_loader : DataLoader\n",
    "        Training dataloader.\n",
    "    loss_func : BCELoss\n",
    "        Binary cross-entropy loss function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        Metrics of interest.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The ``model.eval()`` is also very important:\n",
    "        - It turns off the dropout layers, which are likely to impair the\n",
    "          validation performance and render it unrealistically poor.\n",
    "        - It tells the batchnorm layers to _not_ use the batch's statistics,\n",
    "          and to instead use the stats it has built up from the training set.\n",
    "          The model should not \"remember\" anything from the validation set.\n",
    "    - We also protect this function with ``torch.no_grad()``, because having\n",
    "      gradients enable while validating is a pointless waste of\n",
    "      resources — they are only needed for training.\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metrics: Dict[str, float] = {\"loss\": [], \"accuracy\": []}\n",
    "\n",
    "    for batch, targets in valid_loader:\n",
    "        preds = model(batch).softmax(dim=1)\n",
    "\n",
    "        loss = loss_func(preds, targets)\n",
    "\n",
    "        metrics[\"loss\"].append(loss.item())\n",
    "        metrics[\"accuracy\"].append(batch_level_accuracy(preds, targets))\n",
    "\n",
    "    return {key: mean(val) for key, val in metrics.items()}\n",
    "\n",
    "\n",
    "@no_grad()\n",
    "def batch_level_accuracy(preds: Tensor, targets: Tensor):\n",
    "    \"\"\"Compute the batch-level accuracy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : Tensor\n",
    "        The model's predictions.\n",
    "    targets : Tensor\n",
    "        The corresponding labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The batch-level accuracy.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This function assumes the ``preds`` have had the softmax\n",
    "      applied to them along dimension 1, and that the predicted\n",
    "      class is therefore ``preds.argmax(dim=1)``.\n",
    "\n",
    "    \"\"\"\n",
    "    return (preds.argmax(dim=1) == targets.argmax(dim=1)).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Training, extracting and plotting metrics\n",
    "\n",
    "- Now we can train our model for a specified number of epochs.\n",
    "  - During each epoch the model \"sees\" each training item once.\n",
    "- Append the training and validation metrics to a list.\n",
    "- Turn them into a ``pandas.DataFrame``\n",
    "  - Note: You can turn a ``List[Dict{str, float}]``, say ``my_list`` into a ``DataFrame`` with ``DataFrame(my_list)``.\n",
    "- Use Matplotlib to plot the training and validation metrics as a function of the number of epochs.\n",
    "\n",
    "We will begin the code block together before you complete it independently.  \n",
    "After some time we will go through the solution together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "epochs = 200\n",
    "print_interval = 25\n",
    "\n",
    "\n",
    "train_metrics, valid_metrics = [], []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    if epoch % print_interval == 0:\n",
    "        if epoch != 0:\n",
    "            print(\n",
    "                f\"Epoch {max(epoch - print_interval, 0)}-{epoch} time: {perf_counter() - tic:.6f} seconds\"\n",
    "            )\n",
    "        tic = perf_counter()\n",
    "\n",
    "    train_metrics.append(train_one_epoch(model, train_loader, optimiser, loss_func))\n",
    "\n",
    "    valid_metrics.append(validate_one_epoch(model, valid_loader, loss_func))\n",
    "\n",
    "    stop_time = perf_counter()\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "train_metrics = DataFrame(train_metrics)\n",
    "valid_metrics = DataFrame(valid_metrics)\n",
    "\n",
    "metrics = train_metrics.join(valid_metrics, lsuffix=\"_train\", rsuffix=\"_valid\")\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11: Visualise some results\n",
    "\n",
    "Let's do this part together—though feel free to make a start on your own if you have completed the previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import linspace\n",
    "\n",
    "\n",
    "quantities = [\"loss\", \"accuracy\"]\n",
    "splits = [\"train\", \"valid\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "for axis, quant in zip(axes.ravel(), quantities):\n",
    "    for split in splits:\n",
    "        key = f\"{quant}_{split}\"\n",
    "        axis.plot(\n",
    "            linspace(1, epochs, epochs),\n",
    "            metrics[key],\n",
    "            \"-o\",\n",
    "            ms=1.5,\n",
    "            label=split.capitalize(),\n",
    "        )\n",
    "    axis.set_ylabel(quant.capitalize(), fontsize=15)\n",
    "\n",
    "for axis in axes.ravel():\n",
    "    axis.legend(fontsize=15)\n",
    "    axis.set_ylim(bottom=0.0, top=1.0)\n",
    "    axis.set_xlim(left=1, right=epochs)\n",
    "    axis.set_xlabel(\"Epoch\", fontsize=15)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Run the net on 'new' inputs\n",
    "\n",
    "We have built and trained a net, and evaluated and visualised its performance. However, how do we now utilise it going forward?\n",
    "\n",
    "Here we construct some 'new' input data and use our trained net to infer the species. Whilst this is relatively straightforward there is still some work required to transform the outputs from the net to a meaningful result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import no_grad\n",
    "\n",
    "# Construct a tensor of inputs to run the model over\n",
    "demo_input = tensor(\n",
    "    [\n",
    "        [42.9, 13.1, 5000.0, 215.0, 0.0],\n",
    "        [33.6, 11.3, 2000.0, 211.0, 1.0],\n",
    "    ]\n",
    ")\n",
    "print(f\"Raw input:\\n{demo_input}\\n\")\n",
    "\n",
    "# Place model in eval mode and run over inputs with no_grad\n",
    "model.eval()\n",
    "with no_grad():\n",
    "    demo_output = model(demo_input).softmax(dim=1)\n",
    "\n",
    "# Print the raw output from the net\n",
    "print(f\"Raw output:\\n{demo_output}\\n\")\n",
    "\n",
    "# Transform the raw output back to human-readable format\n",
    "print(f\"Predicted species:\\n{[target_names[val.argmax()] for val in demo_output]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
