{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Penguin regression with PyTorch\n",
    "\n",
    "<img src=\"https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png\" width=\"750\" />\n",
    "\n",
    "\n",
    "Artwork by @allison_horst\n",
    "\n",
    "In this exercise, we will again use the [``palmerpenguins``](https://github.com/mcnakhaee/palmerpenguins) data to continue our exploration of PyTorch.\n",
    "\n",
    "We will use the same dataset object as before, but this time we'll take a look at a regression problem: predicting the mass of a penguin given other physical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: look at the data\n",
    "In the following code block, we import the ``load_penguins`` function from the ``palmerpenguins`` package.\n",
    "\n",
    "- Load the penguin data as you did before.\n",
    "- This time, consider which features we might like to use to predict a penguin's mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_workshop\n",
    "from palmerpenguins import load_penguins\n",
    "\n",
    "data = load_penguins()\n",
    "\n",
    "# Note: ``pd.DataFrame.describe`` is a useful function for giving an overview\n",
    "# of what a ``pd.DataFrame`` contains.\n",
    "print(data.describe())\n",
    "\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now discuss the features we will use to classify the penguins' species, and populate the following list together:\n",
    "\n",
    "#### Let's use:\n",
    "\n",
    "- ``\"species\"``\n",
    "    - Perhaps the most relevant aspect from which to estimate mass.\n",
    "- ``\"sex\"``\n",
    "    - Biologically relevant.\n",
    "- ``\"bill_length_mm\"``\n",
    "    - Biologically relevant.\n",
    "- ``\"bill_depth_mm\"``\n",
    "    - Biologically relevant.\n",
    "- ``flipper_length_mm``\n",
    "    - Biologically relevant.\n",
    "\n",
    "#### Let's reject\n",
    "- ``\"island\"``\n",
    "    - While island could be predictive if dominated by a particular species it would be acting as a proxy and we have already included species as an input feature. \n",
    "- ``\"year\"``\n",
    "    - This feature could also be important: then behaviour of certain species may be changing in response to time-dependent environmental factors such as melting ice. It does however seem like the least biologically-relevant feature, and the most likely source of bias, so we reject it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: creating a ``torch.utils.data.Dataset``\n",
    "\n",
    "As before, we need to create PyTorch ``Dataset`` objects to supply data to our neural network.  \n",
    "Since we have already created and explored the ``PenguinDataset`` class there is nothing else to do here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Obtaining training and validation datasets.\n",
    "\n",
    "- Instantiate the penguin dataloader.\n",
    "  - Make sure you supply the correct column titles for the features and the targets.\n",
    "  - Remember, the target is now mass, not the species!\n",
    "- Iterate over the dataset\n",
    "    - Hint:\n",
    "        ```python\n",
    "        for features, targets in dataset:\n",
    "            # print the features and targets here\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_workshop import PenguinDataset\n",
    "\n",
    "features = [\n",
    "    \"sex\",\n",
    "    \"bill_length_mm\",\n",
    "    \"bill_depth_mm\",\n",
    "    \"flipper_length_mm\",\n",
    "]\n",
    "\n",
    "data_set = PenguinDataset(\n",
    "    input_keys=features,\n",
    "    target_keys=[\"body_mass_g\"],\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "for _, (input_feats, target) in zip(range(20), data_set):\n",
    "    print(input_feats, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Applying transforms to the data\n",
    "\n",
    "As in the previous exercise, the raw inputs and targets need transforming to ``torch.Tensor``s before they can be passed to a neural network.  \n",
    "We will again use ``torchvision.transforms.Compose`` to take a list of callable objects and apply them to the incoming data.\n",
    "\n",
    "Because the raw units of mass are in grams, the numbers are quite large. This can encumber the model's predictive power. A sensible way to address this is to normalise targets using statistics from the training set. The most common form of normalisation is to subtract the mean and divide by the standard deviation (of the training set). However here, for the sake of simplicity, we will just scale the mass by dividing by the mean of the training set.\n",
    "\n",
    "Note that this means that the model will now be trained to predict masses as fractions of the training mean.\n",
    "\n",
    "We grab the mean of the training split in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = PenguinDataset(features, [\"body_mass_g\"], train=True)\n",
    "\n",
    "training_mean = train_set.split.body_mass_g.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our real training and validation set, and supply transforms as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor, float32, eye\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "\n",
    "# Apply the transforms we need to the PenguinDataset to get out inputs\n",
    "# targets as Tensors.\n",
    "\n",
    "\n",
    "def get_input_transforms() -> Compose:\n",
    "    \"\"\"Return transforms which map from raw inputs to tensors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Compose\n",
    "        A composition of transforms (callable functions) to map the tuple\n",
    "        of input features (``Tuple[float, ...]``) to a ``torch.Tensor``.\n",
    "\n",
    "    \"\"\"\n",
    "    return Compose([lambda x: tensor(x, dtype=float32)])\n",
    "\n",
    "\n",
    "def get_target_tfms() -> Compose:\n",
    "    \"\"\"Return transforms which map from the raw targets to tensors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Compose\n",
    "        A composition of transforms (callable functions) to map the tuple\n",
    "        of target features (``Tuple[str]``) to a ``torch.Tensor``.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Note the rescaling by dividing out the training mean.\n",
    "\n",
    "    \"\"\"\n",
    "    return Compose([lambda x: tensor(x, dtype=float32) / training_mean])\n",
    "\n",
    "\n",
    "train_set = PenguinDataset(\n",
    "    input_keys=features,\n",
    "    target_keys=[\"body_mass_g\"],\n",
    "    train=True,\n",
    "    x_tfms=get_input_transforms(),\n",
    "    y_tfms=get_target_tfms(),\n",
    ")\n",
    "\n",
    "\n",
    "valid_set = PenguinDataset(\n",
    "    input_keys=features,\n",
    "    target_keys=[\"body_mass_g\"],\n",
    "    train=False,\n",
    "    x_tfms=get_input_transforms(),\n",
    "    y_tfms=get_target_tfms(),\n",
    ")\n",
    "\n",
    "\n",
    "for _, (input_feats, target) in zip(range(5), train_set):\n",
    "    print(input_feats, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Creating ``DataLoaders``—Again!\n",
    "\n",
    "As before, we wrap our ``Dataset``s in ``DataLoader`` before we proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "for batch, targets in valid_loader:\n",
    "    print(batch.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Creating a neural network in PyTorch\n",
    "\n",
    "Previously we created our neural network from scratch, but doing this every time we need to solve a new problem is cumbersome.  \n",
    "Many projects working with the ICCS have codes where the numbers of layers, layer sizes, and other parts of the models are hard-coded from scratch every time!\n",
    "\n",
    "The result is ungainly, non-general, and heavily-duplicated code. Here, we are going to shamelessly punt Jim Denholm's Python repo, [``TorchTools``](https://github.com/jdenholm/TorchTools), which contains generalisations of many commonly-used PyTorch tools, to save save us some time.\n",
    "\n",
    "Here, we can use the ``FCNet`` model, whose documentation lives [here](https://jdenholm.github.io/TorchTools/models.html). This model is a fully-connected neural network with various options for dropout, batch normalisation, and easily-modifiable layers.\n",
    "\n",
    "#### A brief sidebar\n",
    "Note: the repo is pip-installable with\n",
    "```bash\n",
    "pip install git+https://github.com/jdenholm/TorchTools.git\n",
    "```\n",
    "but has already been installed for you in the requirements of this workshop package.\n",
    "\n",
    "It is useful to know you can install Python packages from GitHub using pip. To install specific versions you can use:\n",
    "```bash\n",
    "pip install git+https://github.com/jdenholm/TorchTools.git@v0.1.0\n",
    "```\n",
    "(The famous [segment anything model](https://github.com/facebookresearch/segment-anything) (SAM) published by Facebook Research was released in this way.)\n",
    "\n",
    "One might argue that this is a much better way of making one-off codes available, for example academic codes which might accompany papers, rather than using the global communal package index PyPI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Back to work: let's instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_tools import FCNet\n",
    "\n",
    "model = FCNet(\n",
    "    in_feats=4,\n",
    "    out_feats=1,\n",
    "    hidden_sizes=(16, 8, 4),\n",
    "    input_bnorm=True,\n",
    "    input_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    hidden_bnorm=True,\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Selecting a loss function\n",
    "\n",
    "The previous loss function we chose was appropriate for classification, but _not_ regression.  \n",
    "Here we'll use the mean-squared-error loss, which is more appropriate for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "\n",
    "loss_func = MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Selecting an optimiser\n",
    "\n",
    "``Adam`` is regarded as the king of optimisers: let's use it again, but this time specifying the learning rate.\n",
    "\n",
    "[Adam docs](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimiser and give it the model's parameters.\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimiser = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Writing basic training and validation loops\n",
    "\n",
    "\n",
    "As before, we will write the training loop together and you can then continue with the validation loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from torch.nn import Module\n",
    "from torch import Tensor, no_grad\n",
    "from numpy import mean, sqrt\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: Module,\n",
    "    train_loader: DataLoader,\n",
    "    optimiser: Adam,\n",
    "    loss_func: MSELoss,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Train ``model`` for once epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Module\n",
    "        The neural network.\n",
    "    train_loader : DataLoader\n",
    "        Training dataloader.\n",
    "    optimiser : Adam\n",
    "        The optimiser.\n",
    "    loss_func : MSELoss\n",
    "        Mean squared error loss function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        A dictionary of metrics.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The ``model.train()`` is very important:\n",
    "        - it turns on the dropout layers.\n",
    "        - it tells the batch norm layers to use the incoming statistics, and\n",
    "          let them contribute to their \"memory\".\n",
    "\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    metrics: Dict[str, float] = {\"loss\": []}\n",
    "\n",
    "    for batch, targets in train_loader:\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        preds = model(batch)\n",
    "\n",
    "        loss = loss_func(preds, targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimiser.step()\n",
    "\n",
    "        metrics[\"loss\"].append(loss.item())\n",
    "\n",
    "    return {key: mean(val) for key, val in metrics.items()}\n",
    "\n",
    "\n",
    "@no_grad()\n",
    "def validate_one_epoch(\n",
    "    model: Module,\n",
    "    valid_loader: DataLoader,\n",
    "    loss_func: MSELoss,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Validate ``model`` for a single epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Module\n",
    "        The neural network.\n",
    "    valid_loader : DataLoader\n",
    "        Validation dataloader.\n",
    "    loss_func : MSELoss\n",
    "        Mean squared error loss function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        Metrics of interest.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The ``model.eval()`` is also very important:\n",
    "        - It turns off the dropout layers, which are likely to impair the\n",
    "          validation performance and render it unrealistically poor.\n",
    "        - It tells the batchnorm layers to _not_ use the batch's statistics,\n",
    "          and to instead use the stats it has built up from the training set.\n",
    "          The model should not \"remember\" anything from the validation set.\n",
    "    - We also protect this function with ``torch.no_grad()``, because having\n",
    "      gradients enable while validating is a pointless waste of resources—they\n",
    "      are only needed for training.\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metrics: Dict[str, float] = {\"loss\": []}\n",
    "\n",
    "    for batch, targets in valid_loader:\n",
    "        preds = model(batch)\n",
    "\n",
    "        loss = loss_func(preds, targets)\n",
    "\n",
    "        metrics[\"loss\"].append(loss.item())\n",
    "\n",
    "    return {key: mean(val) for key, val in metrics.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Training and extracting metrics\n",
    "\n",
    "- Now we can train our model for a specified number of epochs.\n",
    "  - During each epoch the model \"sees\" each training item once.\n",
    "- Append the training and validation metrics to a list.\n",
    "- Turm them into a ``pandas.DataFrame``\n",
    "  - Note: You can turn a ``List[Dict[str, float]]``, say ``my_list`` into a ``DataFrame`` with ``DataFrame(my_list)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "from pandas import DataFrame\n",
    "\n",
    "epochs = 300\n",
    "print_interval = 25\n",
    "\n",
    "train_metrics, valid_metrics = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    if epoch % print_interval == 0:\n",
    "        if epoch != 0:\n",
    "            print(\n",
    "                f\"Epoch {max(epoch - print_interval, 0)}-{epoch} time: {perf_counter() - tic:.6f} seconds\"\n",
    "            )\n",
    "        tic = perf_counter()\n",
    "\n",
    "    train_metrics.append(train_one_epoch(model, train_loader, optimiser, loss_func))\n",
    "\n",
    "    valid_metrics.append(validate_one_epoch(model, valid_loader, loss_func))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "train_metrics = DataFrame(train_metrics)\n",
    "valid_metrics = DataFrame(valid_metrics)\n",
    "\n",
    "metrics = train_metrics.join(valid_metrics, lsuffix=\"_train\", rsuffix=\"_valid\")\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11: Plotting metrics\n",
    "\n",
    "- Use Matplotlib to plot the training and validation metrics as a function of the number of epochs.\n",
    "- Does this allow us to interpret performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import linspace\n",
    "\n",
    "\n",
    "quantities = [\"loss\", \"ratio\"]\n",
    "fig, axis = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "\n",
    "axis.plot(metrics.loss_train, \"-o\", label=\"Train\")\n",
    "axis.plot(metrics.loss_valid, \"-o\", label=\"Valid\")\n",
    "\n",
    "# for axis in axes.ravel():\n",
    "axis.legend(fontsize=15)\n",
    "axis.set_ylim(bottom=0.0, top=1.25)\n",
    "axis.set_xlim(left=0, right=epochs)\n",
    "axis.set_xlabel(\"Epoch\", fontsize=15)\n",
    "axis.set_ylabel(\"Loss (MSE)\", fontsize=15)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this losses have decreased nicely, MSE isn't the most easily interpretable way to evaluate the model's performance. We could instead look at the ratio of the predictions to targets—and in particular, the distribution of this quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import concat\n",
    "from numpy import diff\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "for batch, target in valid_loader:\n",
    "    with no_grad():\n",
    "        predictions.append(model(batch))\n",
    "        targets.append(target)\n",
    "\n",
    "predictions = concat(predictions).flatten()\n",
    "targets = concat(targets).flatten()\n",
    "\n",
    "ratios = predictions / targets\n",
    "\n",
    "fig, axis = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "axis.hist(\n",
    "    ratios,\n",
    "    bins=10,\n",
    "    label=rf\"Mean {ratios.mean():.3f}; Std dev. {predictions.std():.3f}\",\n",
    ")\n",
    "axis.set_xlabel(\"Prediction:target ratio\", fontsize=12)\n",
    "axis.set_ylabel(\"Frequency\", fontsize=12)\n",
    "\n",
    "axis.legend(fontsize=12)\n",
    "\n",
    "axis.set_aspect(0.5 * diff(axis.get_xlim()) / diff(axis.get_ylim()))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
