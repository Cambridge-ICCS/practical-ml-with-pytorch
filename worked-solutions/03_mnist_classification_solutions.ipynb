{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying handwritten digits with a CNN\n",
    "\n",
    "\n",
    "In the previous exercise, we worked with fully-connected neural networks, which are good at handling tabular data, where the inputs and targets are easily presented as vectors.\n",
    "\n",
    "However, in the case of images, or image-like objects, such models are less efficient for reasons we have discussed in the slides. When inputs are images, or image-like data, a more natural choice of model is a convolution neural network—in particularly, a model which uses 2D convolutional layers.\n",
    "\n",
    "Before we start worrying about choosing models, let's first acquaint ourselves with the MNIST data.\n",
    "\n",
    "The first step is to select a directory for the data to live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_root = \"mnist-data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we all set a path this way it will help to maintain consistency throughout this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Access the MNIST dataset.\n",
    "\n",
    "PyTorch has a (sort of) sister Python library for dealing with images: [``Torchvision``](https://pytorch.org/vision/stable/index.html) (take a look at the website for a few minutes).\n",
    "\n",
    "In the previous exercise, we used a custom ``Dataset`` object created specifically for this event, but with ``Torchvision`` come several easy-to-use datasets, one of which is the MNIST digits.\n",
    "\n",
    "- Look at the arguments of the [MNIST datset](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html): what options do we have?\n",
    "- Instantiate the (training) dataset.\n",
    "- Iterate over it: how are the inputs and targets presented to us?\n",
    "- Plot some images, and set their targets as the title, to make sure the data make sense.\n",
    "\n",
    "Note: this section might be challenging for many people, so we will go through it together first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_set = MNIST(root=mnist_root, download=True)\n",
    "\n",
    "print(data_set)\n",
    "\n",
    "img_i, target_i = data_set[0]\n",
    "print(\"\\n\")\n",
    "print(f\"Input is: {img_i}\\n\")\n",
    "print(f\"Target is: {target_i}\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 5)\n",
    "\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    img_i, target_i = data_set[i]\n",
    "    ax.imshow(img_i, cmap=\"inferno\")\n",
    "    ax.set_title(f\"{target_i}\")\n",
    "    ax.set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Task 2\n",
    "\n",
    "As before, we have a dataset which gives us inputs and targets, but we still need to convert the data to ``torch.Tensors``\n",
    "\n",
    "#### Part (a) — raw data to ``Tensor``s\n",
    "\n",
    "Let's do this basic steps first: supply transforms to map between the raw data and ``torch.Tensors``.\n",
    "\n",
    "\n",
    "\n",
    "#### Part (b) — fun with data augmentation\n",
    "\n",
    "Additionally, with image data there are some other consideration we might make:\n",
    "- Are CNNs rotationally invariant?\n",
    "  - If we want our model to work on images which are not of a regular orientation, we must use random rotations as a form of augmentation.\n",
    "- If we train a model on purely black-and-white images, how will it fare on more colourful data?\n",
    "- Go to ``Torchvision``'s [transforms](https://pytorch.org/vision/stable/transforms.html) and look at the available forms of images augmentation.\n",
    "  - Take a few minutes to pick ones you think might be relevant.\n",
    "  - Let's discuss and choose some:\n",
    "    - `RandomRotation` will prevent rotational invariance as discussed in the lectures.\n",
    "      - It will be interesting to see how well the net handles the difference between 6 and 9, however.\n",
    "    - `ColorJitter` applies random variation to the images in terms of hue, saturation, brightness, and contrast. This will enhance feature detection for this dataset.\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor, Compose, RandomRotation, ColorJitter\n",
    "from torch import Tensor, tensor, float32, eye\n",
    "from typing import List, Callable\n",
    "\n",
    "\n",
    "def get_img_tfms(training: bool) -> Compose:\n",
    "    \"\"\"Return a composition of image transforms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    training : bool\n",
    "        Are we training? If ``False``, we are validating.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Compose\n",
    "        A composition of image transforms.\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert the image from being a 2D tensor to have 3 values for each pixel (RGB).\n",
    "    # Since the three values are the same it remains the same grayscale image\n",
    "    tfm_list: List[Callable] = [ToTensor(), lambda x: x.repeat(3, 1, 1)]\n",
    "\n",
    "    if training is True:\n",
    "        # tfm_list.append(???)\n",
    "        tfm_list.append(RandomRotation(180))\n",
    "        tfm_list.append(\n",
    "            ColorJitter(brightness=0.1, hue=0.1, saturation=0.1, contrast=0.1)\n",
    "        )\n",
    "        pass\n",
    "\n",
    "    return Compose(tfm_list)\n",
    "\n",
    "\n",
    "def target_as_tensor(target_idx: int) -> Tensor:\n",
    "    \"\"\"Express the target as a ``torch.Tensor``.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target_idx : int\n",
    "        The ground truth (i.e. the number shown in the image).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor\n",
    "        The target as a one-hot-encoded vector.\n",
    "\n",
    "    \"\"\"\n",
    "    return tensor(target_idx)\n",
    "\n",
    "\n",
    "train_set = MNIST(\n",
    "    mnist_root,\n",
    "    transform=get_img_tfms(training=True),\n",
    "    target_transform=target_as_tensor,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "\n",
    "valid_set = MNIST(\n",
    "    mnist_root,\n",
    "    transform=get_img_tfms(training=False),\n",
    "    target_transform=target_as_tensor,\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "figure, axes = plt.subplots(5, 5, figsize=(15, 15))\n",
    "\n",
    "for idx, axis in enumerate(axes.ravel()):\n",
    "    image, target = train_set[idx]\n",
    "\n",
    "    axis.imshow(image.permute(1, 2, 0), cmap=\"inferno\")\n",
    "    axis.set_title(target)\n",
    "    axis.set_axis_off()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: ``Dataset`` $\\to$ ``DataLoader``\n",
    "\n",
    "As before, wrap the ``Dataset``s in ``DataLoader``s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "for _, (batch, targets) in zip(range(10), valid_loader):\n",
    "    print(batch.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Choose a model architecture\n",
    "\n",
    "Torchvision provides a collection of models, [here](https://pytorch.org/vision/stable/models.html).\n",
    "\n",
    "Since we are all using laptops, many of which don't have CUDA-enabled GPUs, we will select a modest neural network than won't melt any of our hardware. Such a network, designed for mobile phones, is [``MOBILENET``](https://pytorch.org/vision/stable/models/mobilenetv3.html).\n",
    "\n",
    "#### Task 4 (a): instantiate the small version of ``MOBILENET``, and print it out.\n",
    "\n",
    "Note:\n",
    "- Torchvision's models can optionally be endowed with pretrained weights (from corresponding models pretrain on the ImageNet dataset).\n",
    "- We can (optionally) supply these weights.\n",
    "  - Using weights from a model, trained on one problem, as an initial condition in another problem, is called transfer learning.\n",
    "  - Why do you think this might be advantageous, even in disparate problems?\n",
    "    - It will already be trained to recognise some generic/abstract features.\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v3_small\n",
    "from torchvision.models import MobileNet_V3_Small_Weights\n",
    "\n",
    "model = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4 (b): overloading the final layer\n",
    "\n",
    "Uh oh, we've hit a problem.\n",
    "\n",
    "- Look at the final linear layer.\n",
    "  - How many output classes are there?\n",
    "  - How many do we need?\n",
    "- We need to \"overload\" the final layer to produce the correct number of output features for our problem. Fortunately this is easy.\n",
    "- Uncomment the code below, choose the correct number of output features, and print the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "\n",
    "model.classifier[3] = Linear(model.classifier[3].in_features, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Set up the remaining PyTorch bits and bobs\n",
    "\n",
    "- We need to choose a loss function appropriate for classification.\n",
    "  - Can you remember what we chose previously?\n",
    "- We need an optimiser, too.\n",
    "  - Remember our friend, Adam?\n",
    "- Instantiate the model and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "loss_func = CrossEntropyLoss()\n",
    "optimiser = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Set the device\n",
    "\n",
    "We could have done this when we created the model, but it's fine to do it here.\n",
    "- If [CUDA](https://pytorch.org/docs/stable/cuda.html) (NVIDIA GPU) is available we will elect to run on that.\\n\",\n",
    "- If [XPU](https://pytorch.org/docs/stable/xpu.html) (Intel GPU) is available we will elect to run on that.\\n\",\n",
    "- Alternatively, if we are on Apple Silicon we can use the [MPS backend](https://developer.apple.com/metal/pytorch/) for GPU acceleration.\n",
    "- If none of the above is available we will resort to using the (slower) CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.cuda as cuda\n",
    "import torch.xpu as xpu\n",
    "from torch.backends import mps\n",
    "\n",
    "if cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif xpu.is_available():\n",
    "    DEVICE =\"xpu\"\n",
    "elif mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(DEVICE)\n",
    "\n",
    "# Note: here the return/assignment to ``_`` is just to suppress the print.\n",
    "# The model is moved onto the correct device in-place.\n",
    "_ = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Writing our training and validation loops\n",
    "\n",
    "As before, we need to write our training and validation loops.\n",
    "\n",
    "- Complete the training loop\n",
    "- Complete the validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from torch import no_grad\n",
    "from torch.nn import Module\n",
    "\n",
    "from numpy import mean\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: Module,\n",
    "    train_loader: DataLoader,\n",
    "    optimiser: Adam,\n",
    "    loss_func: CrossEntropyLoss,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Train the model for a single epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Module\n",
    "        A neural network.\n",
    "    train_loader : DataLoader\n",
    "        The ``DataLoader`` for the training set.\n",
    "    optimiser : Adam\n",
    "        The optimiser to update the model's parameters with.\n",
    "    loss_func : CrossEntropyLoss\n",
    "        Cross-entropy loss function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        A dictionary of metrics.\n",
    "\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    metrics: Dict[str, List[float]] = {\"loss\": [], \"accuracy\": []}\n",
    "\n",
    "    for batch, targets in train_loader:\n",
    "        batch, targets = batch.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        preds = model(batch)\n",
    "\n",
    "        loss = loss_func(preds, targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimiser.step()\n",
    "\n",
    "        metrics[\"loss\"].append(loss.item())\n",
    "        metrics[\"accuracy\"].append(batch_level_accuracy(preds, targets))\n",
    "\n",
    "    return {key: float(mean(val)) for key, val in metrics.items()}\n",
    "\n",
    "\n",
    "@no_grad()\n",
    "def validate_one_epoch(\n",
    "    model: Module,\n",
    "    valid_loader: DataLoader,\n",
    "    loss_func: CrossEntropyLoss,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Train the model for a single epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Module\n",
    "        A neural network.\n",
    "    valid_loader : DataLoader\n",
    "        The ``DataLoader`` for the validation set.\n",
    "    loss_func : CrossEntropyLoss\n",
    "        Cross-entropy loss function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        A dictionary of metrics.\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metrics: Dict[str, List[float]] = {\"loss\": [], \"accuracy\": []}\n",
    "\n",
    "    for batch, targets in valid_loader:\n",
    "        batch, targets = batch.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        preds = model(batch)\n",
    "\n",
    "        loss = loss_func(preds, targets)\n",
    "\n",
    "        metrics[\"loss\"].append(loss.item())\n",
    "        metrics[\"accuracy\"].append(batch_level_accuracy(preds, targets))\n",
    "\n",
    "    return {key: float(mean(val)) for key, val in metrics.items()}\n",
    "\n",
    "\n",
    "@no_grad()\n",
    "def batch_level_accuracy(preds: Tensor, targets: Tensor):\n",
    "    \"\"\"Compute the batch-level accuracy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : Tensor\n",
    "        The model's predictions.\n",
    "    targets : Tensor\n",
    "        The corresponding labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The batch-level accuracy.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This function assumes the ``preds`` have had the softmax applied to them\n",
    "      along dimension 1, and that the predicted class is therefore\n",
    "      ``preds.argmax(dim=1)``.\n",
    "\n",
    "    \"\"\"\n",
    "    return (preds.argmax(dim=1) == targets).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Training and extracting metrics\n",
    "\n",
    "- Now we can train our model for a specified number of epochs.\n",
    "  - During each epoch the model \"sees\" each training item once.\n",
    "- Append the training and validation metrics to a list.\n",
    "- Turn them into a ``pandas.DataFrame``\n",
    "  - Note: You can turn a ``List[Dict[str, float]]``, say ``my_list`` into a ``DataFrame`` with ``DataFrame(my_list)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "train_metrics, valid_metrics = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = perf_counter()\n",
    "\n",
    "    train_metrics.append(train_one_epoch(model, train_loader, optimiser, loss_func))\n",
    "\n",
    "    valid_metrics.append(validate_one_epoch(model, valid_loader, loss_func))\n",
    "\n",
    "    stop_time = perf_counter()\n",
    "\n",
    "    print(f\"Epoch {epoch} time: {stop_time - start_time:.3f} seconds.\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "train_metrics = DataFrame(train_metrics)\n",
    "valid_metrics = DataFrame(valid_metrics)\n",
    "\n",
    "metrics = train_metrics.join(valid_metrics, lsuffix=\"_train\", rsuffix=\"_valid\")\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Plotting metrics\n",
    "\n",
    "Let's plot the training and validation metrics together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import linspace\n",
    "\n",
    "\n",
    "quantities = [\"loss\", \"accuracy\"]\n",
    "splits = [\"train\", \"valid\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "for axis, quant in zip(axes.ravel(), quantities):\n",
    "    for split in splits:\n",
    "        key = f\"{quant}_{split}\"\n",
    "        axis.plot(\n",
    "            linspace(1, epochs, epochs),\n",
    "            metrics[key],\n",
    "            \"-o\",\n",
    "            label=split.capitalize(),\n",
    "        )\n",
    "    axis.set_ylabel(quant.capitalize(), fontsize=15)\n",
    "\n",
    "for axis in axes.ravel():\n",
    "    axis.legend(fontsize=15)\n",
    "    axis.set_ylim(bottom=0.0, top=1.0)\n",
    "    axis.set_xlim(left=1, right=epochs)\n",
    "    axis.set_xlabel(\"Epoch\", fontsize=15)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Visualising some predictions\n",
    "\n",
    "Let's pick some random validation items, predict on them, and visualise the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_inds = [0, 666, 1024, 2048, 5555]\n",
    "valid_inds = [8, 16, 24, 32, 40]\n",
    "\n",
    "# Stick the model on the cpu\n",
    "_ = model.to(\"cpu\")\n",
    "_ = model.eval()\n",
    "\n",
    "figure, axes = plt.subplots(1, len(valid_inds), figsize=(2 * len(valid_inds), 2))\n",
    "\n",
    "for idx, axis in zip(valid_inds, axes.ravel()):\n",
    "    img_tensor, target = valid_set[idx]\n",
    "\n",
    "    with no_grad():\n",
    "        pred = model(img_tensor.unsqueeze(0)).softmax(dim=1).argmax(dim=1).item()\n",
    "\n",
    "    axis.imshow(img_tensor.permute(1, 2, 0))\n",
    "    axis.set_axis_off()\n",
    "    axis.set_title(f\"pred: {pred}, act: {target}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
